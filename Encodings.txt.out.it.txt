Prima degli anni 1960, i computer sono stati utilizzati principalmente per masticazione di numeri, piuttosto che per il testo, e la memoria era estremamente costoso. I computer spesso assegnate solo 6 bit per ogni carattere, consentendo solo 64 characters-  "assegnazione di codici per AZ, az, e 0-9 avrebbe lasciato solo 2 codici: tuttavia ancora lontani. La maggior parte dei computer hanno scelto di non sostenere lettere minuscole. Così, i progetti di testo i primi, come Roberto Busa Index Thomisticus, il Brown Corpus, e altri hanno dovuto ricorrere a convenzioni, come digitando un asterisco precedente lettere in realtà destinati ad essere maiuscole.Fred Brooks di IBM ha sostenuto con forza per andare a 8-bit byte, perché un giorno la gente potrebbe desiderare di elaborare il testo; e ha vinto. Sebbene IBM utilizzato EBCDIC, la maggior parte di testo da allora in poi è venuto a essere codificati in ASCII, con valori da 0 a 31 per i caratteri di controllo (non-stampa), e valori 32-127 per i caratteri grafici come lettere, cifre e segni di punteggiatura. La maggior parte delle macchine memorizzati caratteri 8 bit anziché 7, ignorando la bit rimanente o usarlo come un checksum.La quasi-ubiquità di ASCII è stato di grande aiuto, ma non è riuscito a rispondere alle preoccupazioni internazionali e linguistiche. Il dollaro-segno ("$"), non è stato così utile in Inghilterra, ei caratteri accentati usato in spagnolo, Francesi, tedeschi, e molte altre lingue erano del tutto non disponibili in ASCII (per non parlare di caratteri usati in greco, russo, e la maggior parte lingue orientali). Molti individui, aziende e paesi definiti caratteri extra come needed-  "spesso riassegnazione caratteri di controllo, o utilizzando il valore nella gamma da 128 a 255. Utilizzando valori superiori a 128 conflitti con utilizzando il bit 8 ° come checksum, ma l';utilizzo di checksum gradualmente estinti.Questi caratteri supplementari sono stati codificati in modo diverso in diversi paesi, rendendo i testi indecifrabili senza capire le regole del cedente. Ad esempio, un browser potrebbe visualizzare Â¬A piuttosto che `se ha cercato di interpretare un personaggio come un altro set. L';Organizzazione internazionale per la Standardizzazione (ISO) alla fine ha sviluppato diverse pagine di codici in ISO 8859, per ospitare varie lingue. Basano-Latin Il primo di questi (ISO 8859-1) è conosciuto anche come "Latin-1", e copre le esigenze della maggior parte (non tutti) lingue europee che utilizzano personaggi (non c';era abbastanza spazio per coprire tutti). ISO 2022 poi fornito le convenzioni per "switching" tra diversi set di caratteri a metà del file. Molte altre organizzazioni hanno sviluppato variazioni su questi, e per molti anni di Windows e Computer Macintosh utilizzato variazioni incompatibili.La situazione di codifica del testo è diventato sempre più complesso, portando a sforzi da ISO e dal consorzio Unicode per sviluppare un singolo, la codifica dei caratteri unificata che potrebbe coprire tutti noti (o almeno tutti attualmente conosciuto) lingue. Dopo qualche conflitto, [citazione necessaria] questi sforzi sono stati unificati. Unicode consente attualmente di 1.114.112 valori di codice, e assegna i codici che coprono quasi tutti i moderni sistemi di scrittura del testo, così come molti storici quelli che per molti personaggi non-linguistiche quali dingbats della stampante, simboli matematici, eccIl testo viene considerato testo normale, indipendentemente dalla sua codifica. Per comprendere correttamente o processo che il destinatario deve conoscere (o essere in grado di capire) cosa encoding è stato utilizzato; Tuttavia, non è necessario sapere nulla l';architettura computer utilizzato, o sulle strutture binarie definite da qualsiasi programma (se presente) creati i dati. 